# ONA Platform - Scalability Analysis

## Executive Summary

The ONA Platform v2.0 architecture is designed for **horizontal scalability** with target support for:

- **10,000+ concurrent users**
- **1,000+ requests/second**
- **Graphs with 1M+ nodes**
- **Real-time streaming data ingestion**
- **Multi-tenant isolation at scale**

---

## ðŸŽ¯ Scalability Targets by License Tier

| Metric               | Demo | Basic  | Professional | Enterprise |
| -------------------- | ---- | ------ | ------------ | ---------- |
| Max Nodes per Graph  | 100  | 5,000  | 50,000       | 1,000,000+ |
| Concurrent Users     | 100  | 500    | 5,000        | 50,000+    |
| API Calls/Month      | 100  | 10,000 | 100,000      | Unlimited  |
| Data Storage         | N/A  | 10 GB  | 100 GB       | 10+ TB     |
| Streaming Events/Sec | N/A  | N/A    | 1,000        | 100,000+   |

---

## ðŸ—ï¸ Component-by-Component Scaling Strategy

### 1. API Gateway (FastAPI)

#### Current Bottleneck

Single FastAPI instance: ~500-1,000 req/sec

#### Horizontal Scaling Approach

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load Balancer  â”‚ (AWS ALB, Nginx, HAProxy)
â”‚  (Round Robin)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼         â–¼        â–¼        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API   â”‚ â”‚  API   â”‚ â”‚  API   â”‚ â”‚  API   â”‚
â”‚Gateway â”‚ â”‚Gateway â”‚ â”‚Gateway â”‚ â”‚Gateway â”‚
â”‚   #1   â”‚ â”‚   #2   â”‚ â”‚   #3   â”‚ â”‚   #N   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Stateless - Each handles 500-1,000 req/sec
```

#### Implementation

```yaml
# Kubernetes Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-gateway-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-gateway
  minReplicas: 3
  maxReplicas: 50
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
```

#### Scaling Characteristics

- **Linear scaling**: 10 instances = 10,000 req/sec
- **Stateless**: No session affinity required
- **Auto-scaling triggers**: CPU > 70%, Memory > 80%, Request queue depth
- **Scale-up time**: 30-60 seconds (Kubernetes)
- **Scale-down cooldown**: 5 minutes

#### Cost per Instance

- **Small instance** (2 vCPU, 4GB RAM): $50-75/month
- **10 instances**: $500-750/month

---

### 2. Neo4j Graph Database

#### Current Bottleneck

Single Neo4j instance:

- 10K reads/sec
- 1K writes/sec
- Max efficient graph size: ~10M nodes

#### Horizontal Scaling Approach: Neo4j Causal Cluster

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Read Proxy  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ Core 1 â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚  Core 2  â”‚â—„â”€â”€â”€â”€â–ºâ”‚  Core 3  â”‚
    â”‚(Leader)â”‚       â”‚(Follower)â”‚      â”‚(Follower)â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚                  â”‚                  â”‚
        â”‚        Raft Consensus                â”‚
        â”‚                                      â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ Read   â”‚       â”‚  Read    â”‚      â”‚  Read    â”‚
    â”‚Replica â”‚       â”‚ Replica  â”‚      â”‚ Replica  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                  â”‚                  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Read scaling (async replication)
```

#### Scaling Strategy

1. **Core Servers (3-5 minimum)**

   - Handle all writes via Raft consensus
   - Leader election for high availability
   - Write capacity: ~1K-5K writes/sec

2. **Read Replicas (unlimited)**

   - Async replication from cores
   - Handle read-only queries
   - Each replica adds: 10K reads/sec
   - Scale based on read traffic

3. **Sharding by Tenant** (for very large deployments)
   ```python
   def get_neo4j_connection(tenant_id: str) -> Driver:
       """Route tenant to appropriate Neo4j shard."""
       shard_id = hash(tenant_id) % NUM_SHARDS
       return neo4j_connections[shard_id]
   ```

#### Vertical Scaling

- **Small**: 4 vCPU, 16GB RAM â†’ 100K nodes
- **Medium**: 8 vCPU, 32GB RAM â†’ 1M nodes
- **Large**: 16 vCPU, 64GB RAM â†’ 10M nodes
- **XLarge**: 32 vCPU, 128GB RAM â†’ 100M+ nodes

#### Performance Optimization

```cypher
-- Indexing for fast lookups
CREATE INDEX tenant_node_idx FOR (n:Node) ON (n.tenant_id, n.id);
CREATE INDEX tenant_edge_idx FOR ()-[r:EDGE]-() ON (r.tenant_id);

-- Query optimization with tenant filter
MATCH (n:Node {tenant_id: $tenant_id})
WHERE n.id IN $node_ids
RETURN n
LIMIT 1000
```

#### Caching Strategy

```python
# Redis cache for hot graphs
def get_graph_with_cache(tenant_id: str, filters: dict) -> dict:
    """Cache preprocessed graphs in Redis."""
    cache_key = f"graph:{tenant_id}:{hash(str(filters))}"

    # Try cache first
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)

    # Query Neo4j
    graph_data = fetch_graph_from_neo4j(tenant_id, filters)

    # Cache for 5 minutes
    redis_client.setex(cache_key, 300, json.dumps(graph_data))

    return graph_data
```

#### Cost

- **Core cluster (3 cores)**: $600-1,200/month
- **Read replica**: $200-400/month each
- **Enterprise license**: $15K-30K/year

**Total for 100K users**: ~$2,000-3,000/month

---

### 3. Apache Kafka (Streaming Data)

#### Current Bottleneck

Single broker: ~10K messages/sec

#### Horizontal Scaling Approach

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Kafka Cluster (3+ brokers)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Broker 1   â”‚  Broker 2   â”‚   Broker 3      â”‚
â”‚             â”‚             â”‚                 â”‚
â”‚  Topic A    â”‚  Topic A    â”‚   Topic A       â”‚
â”‚  Part 0,3   â”‚  Part 1,4   â”‚   Part 2,5      â”‚
â”‚             â”‚             â”‚                 â”‚
â”‚  Topic B    â”‚  Topic B    â”‚   Topic B       â”‚
â”‚  Part 0     â”‚  Part 1     â”‚   Part 2        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²             â–²             â–²
        â”‚             â”‚             â”‚
    Producer      Producer      Producer
    Groups        Groups        Groups
```

#### Partitioning Strategy

```python
# Partition by tenant_id for parallelism
def produce_message(tenant_id: str, data: dict):
    """Produce message to tenant-specific partition."""
    kafka_producer.send(
        topic='graph-updates',
        key=tenant_id.encode('utf-8'),  # Partition key
        value=json.dumps(data).encode('utf-8')
    )

# Consumer groups scale linearly with partitions
# 12 partitions â†’ 12 parallel consumers
```

#### Scaling Characteristics

- **Throughput**: 100K+ messages/sec per broker
- **Partitions**: One consumer per partition (max parallelism)
- **Replication**: 3x for durability
- **Retention**: 7 days default (configurable)

#### Performance Tuning

```properties
# Kafka broker configuration
num.network.threads=8
num.io.threads=16
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
log.segment.bytes=1073741824
log.retention.hours=168
compression.type=snappy
```

#### Cost

- **3-broker cluster**: $600-900/month
- **Per additional broker**: $200-300/month
- **100K events/sec capacity**: 5-6 brokers = $1,200-1,800/month

---

### 4. PostgreSQL (Licenses & Metadata)

#### Current Bottleneck

Single instance: 5K-10K queries/sec

#### Vertical Scaling (Primary Strategy)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       PostgreSQL Primary            â”‚
â”‚    (All writes, some reads)         â”‚
â”‚   16 vCPU, 64GB RAM, NVMe SSD       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Streaming replication
        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼             â–¼          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Read   â”‚   â”‚ Read   â”‚  â”‚ Read   â”‚
    â”‚Replica â”‚   â”‚Replica â”‚  â”‚Replica â”‚
    â”‚   #1   â”‚   â”‚   #2   â”‚  â”‚   #N   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Read Scaling with Replicas

```python
# Read routing
def query_database(sql: str, is_write: bool = False):
    """Route queries to primary or replica."""
    if is_write:
        return primary_db.execute(sql)
    else:
        # Round-robin across read replicas
        replica = get_next_replica()
        return replica.execute(sql)
```

#### Connection Pooling

```python
# PgBouncer for connection pooling
# 1000s of app connections â†’ 100 DB connections
SQLALCHEMY_POOL_SIZE = 20
SQLALCHEMY_MAX_OVERFLOW = 80
SQLALCHEMY_POOL_RECYCLE = 3600
SQLALCHEMY_POOL_PRE_PING = True
```

#### Partitioning for Large Datasets

```sql
-- Partition audit logs by month
CREATE TABLE audit_logs (
    id SERIAL,
    account_id VARCHAR(50),
    action VARCHAR(100),
    created_at TIMESTAMP
) PARTITION BY RANGE (created_at);

CREATE TABLE audit_logs_2026_01 PARTITION OF audit_logs
    FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');

-- Partition by account_id for tenant isolation
CREATE TABLE datasets (
    id SERIAL,
    account_id VARCHAR(50),
    data JSONB
) PARTITION BY HASH (account_id);
```

#### Cost

- **Primary** (16 vCPU, 64GB): $400-600/month
- **Read replica**: $200-300/month each
- **For 10K users**: 1 primary + 2 replicas = $800-1,200/month

---

### 5. MongoDB (Cached Graph Data)

#### Horizontal Scaling: Sharding

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Mongos     â”‚
                 â”‚   (Router)   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ Shard 1 â”‚   â”‚ Shard 2  â”‚   â”‚ Shard 3  â”‚
    â”‚(Replica â”‚   â”‚(Replica  â”‚   â”‚(Replica  â”‚
    â”‚  Set)   â”‚   â”‚  Set)    â”‚   â”‚  Set)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    account_id    account_id     account_id
      0000-3333     3333-6666      6666-9999
```

#### Sharding Strategy

```javascript
// Shard on account_id (tenant isolation)
sh.shardCollection("ona.graphs", { account_id: "hashed" });

// Create indexes
db.graphs.createIndex({ account_id: 1, created_at: -1 });
db.graphs.createIndex({ account_id: 1, node_count: 1 });
```

#### Scaling Characteristics

- **Linear scaling**: 3 shards = 3x capacity
- **Auto-balancing**: Chunks migrate automatically
- **Each shard**: 100GB-1TB optimal
- **Read/write distribution**: Parallel across shards

#### Cost

- **3-shard cluster** (9 nodes total): $900-1,500/month
- **Per additional shard**: $300-500/month

---

### 6. Redis (Caching & Rate Limiting)

#### Horizontal Scaling: Redis Cluster

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Master 1 â”‚  â”‚Master 2 â”‚  â”‚Master 3 â”‚
â”‚ Slots   â”‚  â”‚ Slots   â”‚  â”‚ Slots   â”‚
â”‚ 0-5460  â”‚  â”‚5461-    â”‚  â”‚10923-   â”‚
â”‚         â”‚  â”‚10922    â”‚  â”‚16383    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚            â”‚            â”‚
     â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Replica 1â”‚  â”‚Replica 2â”‚  â”‚Replica 3â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Cache Strategies

```python
# 1. Graph data caching (hot data)
def cache_graph(tenant_id: str, graph_data: dict, ttl: int = 300):
    """Cache preprocessed graph for 5 minutes."""
    key = f"graph:{tenant_id}"
    redis_client.setex(key, ttl, json.dumps(graph_data))

# 2. Analytics results caching (expensive computations)
def cache_analytics(tenant_id: str, metrics: dict, ttl: int = 3600):
    """Cache analytics for 1 hour."""
    key = f"analytics:{tenant_id}"
    redis_client.setex(key, ttl, json.dumps(metrics))

# 3. Rate limiting (by license tier)
def check_rate_limit(api_key: str, tier: str) -> bool:
    """Sliding window rate limiter."""
    limits = {
        "demo": 10,      # 10 req/min
        "basic": 100,    # 100 req/min
        "professional": 1000,
        "enterprise": None  # Unlimited
    }

    if limits[tier] is None:
        return True

    key = f"ratelimit:{api_key}"
    count = redis_client.incr(key)
    if count == 1:
        redis_client.expire(key, 60)

    return count <= limits[tier]
```

#### Scaling Characteristics

- **Throughput**: 100K+ ops/sec per master
- **Memory**: 2-64GB per node
- **Replication**: 1-5 replicas per master

#### Cost

- **3-master cluster** (6 nodes): $300-600/month
- **High-memory** (64GB): $500-800/month per node

---

### 7. Celery Workers (Async Processing)

#### Horizontal Scaling

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Task Queue (Redis)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼           â–¼           â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Celery  â”‚  â”‚Celery  â”‚  â”‚Celery  â”‚ â”‚Celery  â”‚
â”‚Worker 1â”‚  â”‚Worker 2â”‚  â”‚Worker 3â”‚ â”‚Worker Nâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  Graph      Graph       Graph       Graph
Analytics  Analytics   Analytics   Analytics
```

#### Worker Specialization

```python
# Separate queues for different task types
@celery.task(queue='analytics')
def calculate_centrality(graph_data: dict):
    """CPU-intensive graph analytics."""
    # Uses NetworkX, can take minutes
    pass

@celery.task(queue='batch_import')
def process_large_file(file_path: str):
    """I/O-intensive file processing."""
    # Reads large files, writes to Neo4j
    pass

@celery.task(queue='exports')
def generate_pdf_report(tenant_id: str):
    """Generate reports."""
    pass
```

#### Auto-scaling Configuration

```yaml
# Kubernetes autoscaling for Celery workers
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: celery-workers-hpa
spec:
  scaleTargetRef:
    kind: Deployment
    name: celery-workers
  minReplicas: 5
  maxReplicas: 100
  metrics:
    - type: External
      external:
        metric:
          name: redis_queue_length
        target:
          type: Value
          value: "100" # Scale when queue > 100 tasks
```

#### Cost

- **Worker instance** (4 vCPU, 8GB): $100-150/month
- **10 workers**: $1,000-1,500/month
- **Scales to 100+** during high load

---

## ðŸš€ Real-World Scaling Scenarios

### Scenario 1: Small Deployment (100 users)

**Infrastructure:**

- 2 API Gateway instances: $100-150/month
- 1 Neo4j single instance: $300-400/month
- 1 PostgreSQL instance: $200-300/month
- 1 MongoDB instance: $100-150/month
- 1 Redis instance: $50-100/month
- 2 Celery workers: $200-300/month

**Total: ~$1,000-1,500/month**

**Performance:**

- 200-400 concurrent users
- 100K-200K nodes per graph
- 500-1,000 req/sec

---

### Scenario 2: Medium Deployment (5,000 users)

**Infrastructure:**

- 10 API Gateway instances: $500-750/month
- Neo4j cluster (3 cores + 2 replicas): $1,500-2,500/month
- PostgreSQL (1 primary + 2 replicas): $800-1,200/month
- MongoDB cluster (3 shards): $900-1,500/month
- Redis cluster (3 masters): $300-600/month
- Kafka cluster (3 brokers): $600-900/month
- 10 Celery workers: $1,000-1,500/month

**Total: ~$5,600-9,000/month**

**Performance:**

- 5,000-10,000 concurrent users
- 1M-10M nodes per graph
- 5,000-10,000 req/sec
- 10K streaming events/sec

---

### Scenario 3: Enterprise Deployment (50,000+ users)

**Infrastructure:**

- 50 API Gateway instances: $2,500-3,750/month
- Neo4j cluster (5 cores + 10 replicas): $6,000-9,000/month
- PostgreSQL (1 primary + 5 replicas): $2,000-3,000/month
- MongoDB cluster (12 shards): $3,600-6,000/month
- Redis cluster (6 masters): $600-1,200/month
- Kafka cluster (9 brokers): $1,800-2,700/month
- 50 Celery workers: $5,000-7,500/month
- CDN & Load Balancers: $1,000-2,000/month

**Total: ~$22,500-35,000/month**

**Performance:**

- 50,000+ concurrent users
- 100M+ nodes per graph
- 50,000+ req/sec
- 100K+ streaming events/sec

---

## âš¡ Performance Bottlenecks & Solutions

### Bottleneck 1: Large Graph Rendering

**Problem**: React browser crashes with 50K+ nodes

**Solutions:**

1. **Server-side simplification**

   ```python
   def simplify_graph_for_viz(graph: nx.Graph, max_nodes: int = 5000):
       """Reduce graph size for visualization."""
       if graph.number_of_nodes() <= max_nodes:
           return graph

       # Keep highest degree nodes
       centrality = nx.degree_centrality(graph)
       top_nodes = sorted(centrality, key=centrality.get, reverse=True)[:max_nodes]
       return graph.subgraph(top_nodes)
   ```

2. **Progressive loading**

   ```typescript
   // Load graph in chunks
   const loadGraphInChunks = async (graphId: string) => {
     const chunk1 = await api.fetchNodes(graphId, 0, 1000);
     renderGraph(chunk1);

     const chunk2 = await api.fetchNodes(graphId, 1000, 2000);
     updateGraph(chunk2);
   };
   ```

3. **WebGL rendering** (react-force-graph already uses this)
   - Handles 10K+ nodes smoothly
   - Hardware accelerated

---

### Bottleneck 2: Neo4j Query Performance

**Problem**: Complex graph traversals timeout

**Solutions:**

1. **Query optimization**

   ```cypher
   // âŒ SLOW: No index usage
   MATCH (n)-[r*1..5]-(m)
   WHERE n.tenant_id = $tenant_id
   RETURN n, r, m

   // âœ… FAST: Use indexes, limit depth
   MATCH (n:Node {tenant_id: $tenant_id})
   WHERE n.id IN $node_ids
   MATCH (n)-[r*1..3]-(m:Node {tenant_id: $tenant_id})
   RETURN n, r, m
   LIMIT 1000
   ```

2. **Denormalization**

   ```cypher
   // Store precomputed metrics on nodes
   MATCH (n:Node {tenant_id: $tenant_id})
   SET n.degree = size((n)--())
   SET n.clustering = $clustering_value
   ```

3. **Materialized views in MongoDB**
   ```python
   # Cache expensive graph queries
   def get_neighborhood(node_id: str, tenant_id: str):
       # Check cache
       cached = mongo.graphs.find_one({
           "tenant_id": tenant_id,
           "node_id": node_id,
           "type": "neighborhood"
       })
       if cached:
           return cached["data"]

       # Query Neo4j, cache result
       data = query_neo4j_neighborhood(node_id, tenant_id)
       mongo.graphs.insert_one({
           "tenant_id": tenant_id,
           "node_id": node_id,
           "type": "neighborhood",
           "data": data,
           "expires_at": datetime.now() + timedelta(hours=1)
       })
       return data
   ```

---

### Bottleneck 3: Kafka Consumer Lag

**Problem**: Streaming events pile up, lag increases

**Solutions:**

1. **Scale consumers to match partitions**

   ```python
   # 12 partitions â†’ 12 consumers max
   # Scale consumer group instances
   KAFKA_PARTITIONS = 24  # Double for growth
   CONSUMER_GROUP_INSTANCES = 24
   ```

2. **Batch processing**

   ```python
   def consume_batch(consumer, batch_size=100):
       """Process messages in batches."""
       messages = []
       for _ in range(batch_size):
           msg = consumer.poll(timeout=0.1)
           if msg:
               messages.append(msg)

       # Bulk insert to Neo4j
       if messages:
           bulk_insert_edges(messages)
   ```

3. **Backpressure handling**
   ```python
   # Pause consumer if downstream is slow
   if neo4j_queue_depth > 10000:
       consumer.pause()
       time.sleep(5)
       consumer.resume()
   ```

---

## ðŸ“Š Load Testing Strategy

### Load Test 1: API Gateway Stress Test

```python
# Using Locust
from locust import HttpUser, task, between

class ONAUser(HttpUser):
    wait_time = between(1, 3)

    @task(3)
    def get_graph(self):
        self.client.get("/api/v1/graph/nodes?tenant_id=test")

    @task(2)
    def get_analytics(self):
        self.client.get("/api/v1/graph/metrics?tenant_id=test")

    @task(1)
    def upload_data(self):
        self.client.post("/api/v1/data/upload", files={"file": ...})

# Run: locust -f load_test.py --users 10000 --spawn-rate 100
```

### Load Test 2: Neo4j Throughput Test

```python
import neo4j
import concurrent.futures

def benchmark_neo4j(num_queries=10000):
    with ThreadPoolExecutor(max_workers=100) as executor:
        futures = [
            executor.submit(run_query, f"tenant_{i % 100}")
            for i in range(num_queries)
        ]
        results = [f.result() for f in futures]

    print(f"Queries/sec: {num_queries / elapsed_time}")
```

### Load Test 3: Kafka Throughput Test

```bash
# Producer performance test
kafka-producer-perf-test.sh \
  --topic graph-updates \
  --num-records 1000000 \
  --record-size 1024 \
  --throughput 100000 \
  --producer-props bootstrap.servers=localhost:9092

# Consumer performance test
kafka-consumer-perf-test.sh \
  --topic graph-updates \
  --messages 1000000 \
  --threads 12 \
  --bootstrap-server localhost:9092
```

---

## ðŸŽ¯ Monitoring & Alerting for Scale

### Key Metrics to Track

#### Application Metrics

- API response time (p50, p95, p99)
- Requests per second
- Error rate (4xx, 5xx)
- Cache hit rate

#### Database Metrics

- Neo4j query latency
- PostgreSQL connection pool utilization
- MongoDB shard distribution
- Redis memory usage

#### Infrastructure Metrics

- CPU utilization (per service)
- Memory usage
- Network throughput
- Disk I/O

### Alerting Thresholds

```yaml
# Prometheus alert rules
groups:
  - name: ona_scaling
    rules:
      - alert: HighAPILatency
        expr: histogram_quantile(0.95, http_request_duration_seconds) > 2
        for: 5m

      - alert: Neo4jSlowQueries
        expr: neo4j_cypher_query_duration_seconds > 10
        for: 2m

      - alert: KafkaConsumerLag
        expr: kafka_consumer_lag > 10000
        for: 5m

      - alert: HighCPUUsage
        expr: cpu_usage_percent > 85
        for: 10m
```

---

## ðŸ’° Cost Scaling Analysis

### Cost vs. Users (Monthly)

| Users   | Infrastructure | Neo4j   | Total   |
| ------- | -------------- | ------- | ------- |
| 100     | $700           | $300    | $1,000  |
| 1,000   | $2,000         | $800    | $2,800  |
| 5,000   | $4,100         | $1,500  | $5,600  |
| 10,000  | $6,500         | $2,500  | $9,000  |
| 50,000  | $16,500        | $6,000  | $22,500 |
| 100,000 | $28,000        | $10,000 | $38,000 |

### Cost Optimization Strategies

1. **Reserved Instances** (30-50% savings)
2. **Spot Instances** for Celery workers (60-70% savings)
3. **Auto-scaling** (scale down during off-hours)
4. **CDN** for static assets (reduce API load)
5. **Data lifecycle** (archive old data to cheaper storage)

---

## âœ… Scalability Checklist

### Before Going to Production

- [ ] Load test to 2x expected peak traffic
- [ ] Configure auto-scaling for all services
- [ ] Set up monitoring and alerting
- [ ] Enable database replication
- [ ] Configure CDN for frontend assets
- [ ] Implement rate limiting by tier
- [ ] Test disaster recovery procedures
- [ ] Document scaling playbooks
- [ ] Set up cost monitoring and budgets

### Post-Launch Monitoring

- [ ] Track growth metrics weekly
- [ ] Review performance dashboards daily
- [ ] Analyze slow queries monthly
- [ ] Optimize hot paths quarterly
- [ ] Review infrastructure costs monthly
- [ ] Plan capacity 6 months ahead

---

## ðŸš€ Conclusion

The ONA Platform architecture is designed for **horizontal scalability** across all tiers:

âœ… **Linear scaling**: Add more instances = more capacity  
âœ… **No single points of failure**: All services clustered  
âœ… **Auto-scaling**: Responds to load automatically  
âœ… **Multi-tenant efficient**: Isolated but shared infrastructure  
âœ… **Cost-effective**: Pay only for what you use

### Growth Path

```
   100 users â†’ 1K users â†’ 10K users â†’ 100K users
      â†“           â†“           â†“            â†“
   $1K/mo     $3K/mo      $9K/mo      $38K/mo
   1 region   1 region    2 regions   Global
```

**Expected timeline to scale**: 30-60 seconds (automated)  
**Maximum theoretical scale**: Millions of users across global regions

---

**Last Updated**: January 6, 2026  
**Version**: 1.0
